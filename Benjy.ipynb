{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "953911a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T18:40:20.840150Z",
     "start_time": "2023-06-16T18:40:19.426264Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tabula\n",
    "from tabula.io import read_pdf\n",
    "\n",
    "# read all pages of pdf document\n",
    "file_name = \"Hempstead.pdf\"\n",
    "dfs = tabula.io.read_pdf(file_name, pages=f'1-10', lattice=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "9ca22daf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T04:14:21.473837Z",
     "start_time": "2023-06-18T04:11:02.414765Z"
    }
   },
   "outputs": [],
   "source": [
    "import tabula\n",
    "\n",
    "all_dfs = pd.DataFrame()\n",
    "for x in range(12):\n",
    "    dfs = tabula.read_pdf(\"Hempstead.pdf\",pages=f'{x*800+1}-{(x+1)*800}', lattice=True)\n",
    "    df = pd.concat(dfs)\n",
    "    all_dfs = pd.concat([all_dfs, df])\n",
    "last_df = tabula.read_pdf(\"Hempstead.pdf\",pages='9601-10120', lattice=True)\n",
    "last_df = pd.concat(last_df)\n",
    "all_dfs = pd.concat([all_dfs, last_df])\n",
    "\n",
    "all_dfs.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "6ab1ebcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T04:15:18.860463Z",
     "start_time": "2023-06-18T04:15:18.778831Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['index'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[537], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m new_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(all_dfs\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m2\u001b[39m:])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Shift the values of columns 4-10 to become columns 3-9\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m all_dfs\u001b[38;5;241m.\u001b[39mloc[null_rows, new_columns] \u001b[38;5;241m=\u001b[39m \u001b[43mall_dfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnull_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# drop two columns\u001b[39;00m\n\u001b[0;32m     12\u001b[0m all_dfs \u001b[38;5;241m=\u001b[39m all_dfs\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m\"\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1067\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1065\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1066\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[1;32m-> 1067\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1069\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1254\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;66;03m# ugly hack for GH #836\u001b[39;00m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take_opportunity(tup):\n\u001b[1;32m-> 1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_multi_take\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple_same_dim(tup)\n",
      "File \u001b[1;32m~\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1205\u001b[0m, in \u001b[0;36m_LocIndexer._multi_take\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;124;03mCreate the indexers for the passed tuple of keys, and\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;124;03mexecutes the take operation. This allows the take operation to be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;124;03mvalues: same type as the object being indexed\u001b[39;00m\n\u001b[0;32m   1203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;66;03m# GH 836\u001b[39;00m\n\u001b[1;32m-> 1205\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_AXIS_ORDERS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(d, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1206\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;124;03mCreate the indexers for the passed tuple of keys, and\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;124;03mexecutes the take operation. This allows the take operation to be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;124;03mvalues: same type as the object being indexed\u001b[39;00m\n\u001b[0;32m   1203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;66;03m# GH 836\u001b[39;00m\n\u001b[0;32m   1205\u001b[0m d \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m-> 1206\u001b[0m     axis: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1207\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (key, axis) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tup, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_AXIS_ORDERS)\n\u001b[0;32m   1208\u001b[0m }\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(d, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1432\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1429\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1430\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1432\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32m~\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6113\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6111\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6113\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6115\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6117\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6176\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6175\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6176\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['index'] not in index\""
     ]
    }
   ],
   "source": [
    "test_dfs = all_dfs.copy()\n",
    "# Find rows with null values in the third column\n",
    "null_rows = all_dfs[all_dfs[\"Last Known Owner Name\\rLast Known Mailing Address\"].isnull()].index.to_list()\n",
    "\n",
    "# Slice the DataFrame to keep the first two columns and columns 4-10\n",
    "new_columns = ['index'] + list(all_dfs.columns[2:])\n",
    "\n",
    "# Shift the values of columns 4-10 to become columns 3-9\n",
    "all_dfs.loc[null_rows, new_columns] = all_dfs.loc[null_rows, new_columns].shift(-1, axis=1)\n",
    "\n",
    "# drop two columns\n",
    "all_dfs = all_dfs.drop([\"index\",\"Unnamed: 0\"], axis=1)\n",
    "# Display the updated DataFrame\n",
    "all_dfs.iloc[808:811]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "c019feff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T04:19:11.282955Z",
     "start_time": "2023-06-18T04:19:11.270054Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# concatenate all tables in pdf documents as one dataframe\n",
    "#all_dfs=pd.concat(dfs)\n",
    "\n",
    "# take first 13 columns (some tables had a ghost 14th column)\n",
    "#all_dfs=all_dfs.iloc[:,:13]\n",
    "\n",
    "# rename the dataframe's columns (names were long and poorly formatted)\n",
    "all_dfs.columns=[\"Property Info\",\n",
    "\"Name and Address\",\n",
    "\"Codes\",\n",
    "\"Full Market Value\",\n",
    "\"Land Assessed Value\",\n",
    "\"Total Assessed Value\",\n",
    "\"Exempt Code\",\n",
    "\"Exemption Amount\",\n",
    "\"Village Codes\",\n",
    "\"Rate Codes\",\n",
    "\"Tax District Percent\",\n",
    "\"Total Taxable Value Town\",\n",
    "\"Total Taxable Value County\"]\n",
    "\n",
    "# create one dataframe from the columns that do not need to be expanded (to be joined at the end)\n",
    "all_other_columns_df=test_dfs[[\"Full Market Value\",\n",
    "\"Land Assessed Value\",\n",
    "\"Total Assessed Value\",\n",
    "\"Exempt Code\",\n",
    "\"Exemption Amount\",\n",
    "\"Village Codes\",\n",
    "\"Rate Codes\",\n",
    "\"Tax District Percent\",\n",
    "\"Total Taxable Value Town\",\n",
    "\"Total Taxable Value County\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6328385",
   "metadata": {},
   "source": [
    "## Splitting First column into separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "efcb80ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T04:19:17.278336Z",
     "start_time": "2023-06-18T04:19:17.158499Z"
    }
   },
   "outputs": [],
   "source": [
    "# create multiple columns from the first column's rows (they are read and split on \"\\r\" character)\n",
    "series=all_dfs[\"Property Info\"].apply(lambda x:x.split(\"\\r\"))\n",
    "series_df=pd.DataFrame(data=series)\n",
    "series_df.columns=[\"First_Column\"]\n",
    "series_df[\"length\"]=series_df[\"First_Column\"].apply(lambda x:len(x))\n",
    "all_series=series_df[\"First_Column\"].tolist()\n",
    "\n",
    "# since many cells do not have a value for Lot Grouping, insert a blank value for that place in the list (so the columns aren't distorted)\n",
    "for series in all_series:\n",
    "    if 'acres' in series[2]:\n",
    "        series.insert(1,\"\")\n",
    "\n",
    "# take first 5 items from the list (which will create 5 columns in final dataframe). The bottom of the first cell has long code we don't need.\n",
    "all_series=[series[:5] for series in all_series]\n",
    "\n",
    "# create a dtaframe of 5 columns from the original first column\n",
    "first_column_df=pd.DataFrame(all_series, columns=['Section-Block-Lot', 'Lot Grouping', 'Address','Lot Size','Liber'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af0b2c",
   "metadata": {},
   "source": [
    "## Splitting Second column into separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "ae8b7ce5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T04:19:27.313139Z",
     "start_time": "2023-06-18T04:19:26.974509Z"
    }
   },
   "outputs": [],
   "source": [
    "# split on \"\\r\" character and handle issue with float that cannot be split (this came up in one of the 10,000 pages)\n",
    "address_series=all_dfs['Name and Address'].apply(lambda x:x.split(\"\\r\") if not(isinstance(x, float)) else [\"\",\"\",\"\",\"\"])\n",
    "address_series_df=pd.DataFrame(data=address_series)\n",
    "address_series_df.columns=[\"Second_Column\"]\n",
    "address_series_df[\"length\"]=address_series_df[\"Second_Column\"].apply(lambda x:len(x))\n",
    "all_address_series=address_series_df[\"Second_Column\"].tolist()\n",
    "\n",
    "address_series_df[\"last_value_last_row\"] = address_series_df[\"Second_Column\"].apply(lambda x: x[-1].split(\" \")[-1])\n",
    "address_series_df[\"last_value_second_last_row\"] = address_series_df[\"Second_Column\"].apply(lambda x: x[-2].split(\" \")[-1])\n",
    "address_series_df[\"last_value_third_last_row\"] = address_series_df[\"Second_Column\"].apply(lambda x: x[-3].split(\" \")[-1] if len(x)>2 else \"hello\")\n",
    "\n",
    "\n",
    "\n",
    "# Define the condition using str.match()\n",
    "# Apply the condition and select values from different columns\n",
    "address_series_df[\"zip_code_value\"] = np.where(address_series_df[\"last_value_last_row\"].str.match(r'\\d{5}+'), \\\n",
    "                                             address_series_df[\"last_value_last_row\"],\\\n",
    "                                      np.where(address_series_df[\"last_value_second_last_row\"].str.match(r'\\d{5}+'),\\\n",
    "                                             address_series_df[\"last_value_second_last_row\"],\\\n",
    "                                              address_series_df[\"last_value_third_last_row\"]))\n",
    "\n",
    "(address_series_df[\"zip_code_row\"],address_series_df[\"address_code_row\"]) = \\\n",
    "                    np.where(address_series_df[\"last_value_last_row\"].str.match(r'\\d{5}+'), \\\n",
    "                    (address_series_df[\"Second_Column\"].str.get(-1),address_series_df[\"Second_Column\"].str.get(-2)),\\\n",
    "                     np.where(address_series_df[\"last_value_second_last_row\"].str.match(r'\\d{5}+'),\\\n",
    "                    (address_series_df[\"Second_Column\"].str.get(-2),address_series_df[\"Second_Column\"].str.get(-3)),\\\n",
    "                    (address_series_df[\"Second_Column\"].str.get(-3),address_series_df[\"Second_Column\"].str.get(-4))))\n",
    "\n",
    "address_series_df[\"name\"] = address_series_df[\"Second_Column\"].str.get(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "6e895ddc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T14:19:33.792608Z",
     "start_time": "2023-06-18T14:19:33.736058Z"
    }
   },
   "outputs": [],
   "source": [
    "# split on \"\\r\" character \n",
    "codes_series=all_dfs['Codes'].apply(lambda x:x.split(\"\\r\"))\n",
    "codes_series_df=pd.DataFrame(data=codes_series)\n",
    "codes_series_df.columns=[\"Third_Column\"]\n",
    "all_codes_series=codes_series_df[\"Third_Column\"].tolist()\n",
    "\n",
    "# create a dtaframe of 6 columns from the original third column\n",
    "third_column_df=pd.DataFrame(all_codes_series, columns=['Roll Section', 'SWIS Code',\"Sch SWIS Code\",'School Code','PUC - Class',\"Percent Value\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "f86b5ca1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T14:52:18.379776Z",
     "start_time": "2023-06-18T14:52:18.227991Z"
    }
   },
   "outputs": [],
   "source": [
    "def assessed_value(x):\n",
    "    if type(x) == int:\n",
    "        return x\n",
    "    elif type(x) == float:\n",
    "        return int(x)\n",
    "    else:\n",
    "        return x.split(\"\\r\")[-1]\n",
    "    \n",
    "all_dfs[\"Total Assessed Value\"] = all_dfs[\"Total Assessed Value\"].apply(lambda x: assessed_value(x))\n",
    "\n",
    "pd.concat([address_series_df[['name', 'address_code_row', 'zip_code_row']],\\\n",
    "           all_dfs['Total Assessed Value'],third_column_df],axis=1)\\\n",
    "           .to_csv(\"final_df.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25953fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd3eb89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T14:19:35.353348Z",
     "start_time": "2023-06-18T14:19:35.353348Z"
    }
   },
   "outputs": [],
   "source": [
    "def assessed_value(x):\n",
    "    if type(x) == int:\n",
    "        return x\n",
    "    elif type(x) == float:\n",
    "        return int(x)\n",
    "    else:\n",
    "        return x.split(\"\\r\")[-1]\n",
    "    \n",
    "all_dfs[\"Total Assessed Value\"] = all_dfs[\"Total Assessed Value\"].apply(lambda x: assessed_value(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "8e20bb1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T16:59:58.549876Z",
     "start_time": "2023-06-15T16:59:58.523331Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge first, second, third dataframes with the rest of the columns\n",
    "final_df=pd.concat([first_column_df,second_column_df,third_column_df,all_other_columns_df.reset_index()],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ee0e28",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-25T17:03:44.311Z"
    }
   },
   "outputs": [],
   "source": [
    "#write final dataframe to csv\n",
    "final_df.to_csv(\"All_Properties.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c749654b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "461290d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T13:30:08.357838Z",
     "start_time": "2023-06-18T13:30:08.303100Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(all_dfs[\"Total Assessed Value\"]).to_csv(\"assessed.csv\", index=False)#.apply(lambda y: get_assessed_value(y)).value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
